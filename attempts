batch_size: [64,256] higher = more accurate on training data, less general
kernal size: [3,11] odd number, shouldn't matter much
in_channels = out_channels of previous layer
out_channels = ??? cant find any infomation on best way to set this value








attempts

epochs 10, lr 0.001, batch size 256, nn.NLLLoss()
	layers:
		relu conv(3, 32, 5)
		relu pool(2)    
		relu conv(32,128,5)
		relu pool(2)  
		relu conv(128,256,5)
		
		relu Linear(256 * 9 * 9, 64)
		log_softmax Linear(64, 14)
	Accuracy on 1581 test images: 74.19%





















epochs 10, lr 0.001, batch size 256, nn.NLLLoss()
	layers:
		conv(3, 32, 5)
		pool(2)    
		conv(32,128,5)
		pool(2)  
		conv(128,256,5)
		
		Linear(256 * 9 * 9, 512)
		Linear(512, 256)
		Linear(256, 14)
	Accuracy on 3161 test images: 63.56%


epochs 10, lr 0.001, batch size 256, nn.NLLLoss()
	layers:
		conv(3, 16, 5)    
		conv(16,32,5)    
		conv(32,64,5)
		pool(2)
		Linear(64 * 26 * 26, 512)
		Linear(512, 256)
		Linear(256, 14)
	Accuracy on 3161 test images: 57.10%


epochs 10, lr 0.001, batch size 256, nn.NLLLoss()
	layers:
		conv(3,32,3)
		pool(2)
		conv(32,300,3)
		pool(2)
		Linear(300 * 14 * 14, 64)
		Linear(64, 14)
	Accuracy on 3161 test images: 49.16%



